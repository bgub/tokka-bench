---
description:
globs:
alwaysApply: true
---

# Tokka-Bench Project Guide

## Overview

Tokka-bench is a **tokenizer benchmarking tool** that compares different tokenizers across multiple languages. It evaluates how efficiently different tokenizers (GPT-2, GPT-4, Llama, etc.) handle text from various language families using real-world data from FineWeb-2.

## Core Metrics

- **Bytes per token**: How many UTF-8 bytes each token represents (higher = more efficient)
- **Unique tokens**: How many different token IDs were used (reveals vocabulary coverage and language support ect## Main Package: [src/tokka_bench/](mdc:src/tokka_bench)
- [fast_benchmark.py](mdc:src/tokka_bench/fast_benchmark.py) - **Core fast benchmark**:
  - Multi-tokenizer by default with per-language parallelism
  - Real FineWeb-2/StarCoder streaming
  - Clean JSON output compatible with classic format
- [**init**.py](mdc:src/tokka_bench/__init__.py) - Package initialization

### CLI Interface: [src/tokka_bench/cli.py](mdc:src/tokka_bench/cli.py)

- Fast engine by default; supports any HF model id
- Flags: `tokenizers`, `sample_size`, `max_workers`, `natural_n`, `code_n`

### Data & Configuration

- [src/fineweb-2-languages.csv](mdc:src/fineweb-2-languages.csv) - Language metadata (internal, never changes)
- [pyproject.toml](mdc:pyproject.toml) - Dependencies: transformers, datasets, pandas, omegaconf
- `data/results/` - Output JSON files with benchmark results

## Key Design Principles

### Simplicity & Extensibility

- **No tokenizer registry** - use any HuggingFace model directly
- **Minimal configuration** - only tokenizer name and sample size required
- **Single source of truth** - all logic consolidated in benchmark.py
- **Real data only** - uses actual FineWeb-2 text, no dummy data

### Real-World Data Loading

- **Streaming datasets** - loads exact sample sizes from FineWeb-2
- **Graceful fallback** - uses simple text if dataset loading fails
- **Efficient processing** - accumulates text until target size reached

### Efficiency Interpretation

- **Higher bytes/token = MORE efficient** (each token captures more content)
- **Higher unique tokens = BETTER language coverage** (less byte-fallback)
- Results ranked by efficiency (highest bytes/token first)

## Usage Examples

```bash
# Benchmark GPT-2 with 1MB samples (OmegaConf syntax)
uv run benchmark tokenizer=openai-community/gpt2

# Quick test with smaller samples
uv run benchmark tokenizer=Xenova/gpt-4 sample_size=0.1

# Custom output filename
uv run benchmark tokenizer=meta-llama/Meta-Llama-3-8B output_name=llama-results

# Any HuggingFace model works
uv run benchmark tokenizer=microsoft/DialoGPT-medium
```

## Test Data

Benchmarks run on **top 5 languages by FineWeb-2 size**:

1. Russian (rus-Cyrl) - 1.65TB
2. Mandarin Chinese (cmn-Hani) - 1.34TB
3. German (deu-Latn) - 640.76GB
4. Japanese (jpn-Jpan) - 636.71GB
5. Spanish (spa-Latn) - 554.08GB

## Metric Interpretation

### Bytes per Token

- **Spanish (~3.01)**: Most efficient for GPT-2, excellent Latin script support
- **German (~2.71)**: Strong European language coverage
- **Japanese (~2.25)**: Moderate mixed-script handling
- **Russian (~1.71)**: Poor Cyrillic support, heavy byte-fallback
- **Chinese (~1.40)**: Limited character vocabulary coverage

### Unique Tokens (Language Coverage Quality)

- **Well-supported languages**: 800-1000+ unique tokens (German, Spanish)
- **Moderately supported**: 300-600 unique tokens (Japanese, Chinese)
- **Poorly supported**: 100-300 unique tokens (Russian Cyrillic)
- **Reveals tokenizer bias**: Which languages got priority during training

## Output Format

Results saved as JSON in `data/results/{model_name}.json` with:

- Tokenizer metadata and timestamp
- Per-language metrics: bytes_per_token, total_bytes, total_tokens, unique_tokens
- Language information (ISO codes, scripts, names)
- Sample size configuration

## Technical Implementation

### Clean Exit Handling

- **Simple approach**: Basic cleanup with gc.collect()
- **Standard exit**: Uses sys.exit() instead of os.\_exit()
- **No complex library management**: Removed urllib3/requests cleanup
- **Reliable termination**: No hanging processes

### Error Handling

- **Graceful dataset fallback**: Falls back to simple text if FineWeb-2 fails
- **Exception safety**: Non-fatal cleanup warnings
- **Streaming resilience**: Handles dataset exhaustion properly

## Dependencies

- **transformers**: AutoTokenizer for any HuggingFace model
- **datasets**: Real FineWeb-2 data streaming
- **pandas**: CSV processing for language metadata
- **omegaconf**: CLI configuration management
- **Python 3.9+**: Modern Python features

## Recent Changes

- ✅ Fast multi-tokenizer benchmark (default engine)
- ✅ Added unique_tokens metric (coverage quality)
- ✅ Simplified cleanup and documentation
- ✅ Real data integration (FineWeb-2/StarCoder streaming)
